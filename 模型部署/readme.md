## 在Jetson Xavier NX中使用CUDA加速模型推理
### 实训背景
做移动平台的本地视觉识别，需要有高算力的支持，使用CUDA对模型的推理加速是必不可少，有什么什么优势，balabala..........................
### 实训内容
将深度学习模型成功部署在平台中，并保持较高的推理速度和准确度
### 实训目的及要求
掌握在边缘平台中配置模型运行环境并部署模型的方法
实现2024中国高校智能机器人创意大赛四足专项中锥形桶识别和仪表盘数字检测任务
实现使用CUDA对模型推理进行加速
### 实验软硬件环境
>硬件：Jetson Xavier NX开发板，摄像头（可选）
>软件：
>-  JetPack 4.4
>- Python 3.7
>- onnxruntime 1.11.0
>- opencv-python 4.9.0
>

### 实训操作指南
1. 先训练好自己用来进行识别的模型，由于当前用于视觉识别的模型大部分使用yolo，所以使用YOLOv8s模型为例
- 准备好拍摄的数据集，由于使用机械狗进行图像的识别，建议使用机械狗进行数据集拍摄，在拍摄过程中可以采用将识别对象进行手动旋转、更换场景增加复杂度等方式，采集合适的数据集，一般采集3k~5k张图像。
- 对采集到的数据进行增强处理，由于YOLO自带了旋转、裁剪、模糊等较为通用的数据增强手段，我们需要进行其他的数据增强方式，例如将图像进行空间扭曲，模拟不同角度观察物体；还可以将感兴趣区域裁剪后贴在其他背景图中例如coco2017数据集，人为创造新的环境。使用增强后的数据集训练一个YOLOv8s模型。

2. 部署模型
- 训练好一个模型后，就可以在机械狗上进行模型部署。首先是环境的选择，YOLOv8s本身是使用Pytorch进行推理，但是pytorch原生推理速度较慢，且在当前平台上部署pytorch对环境要求较为严格；TensorRT是Nvidia自家的加速推理框架，推理速度快，但是部署难度较高；ONNX部署难度低，跨平台兼容性好，推理速度稍逊于TensorRT，故选择ONNX作为运行模型的平台，可以用YOLO官方工具将训练所得的`pt`模型转为`onnx`模型。配置ONNX时，需要在Jetson zoo下载onnxruntime-gpu的离线安装包，以适配当前的平台。
- 运行模型需要进行如下几步：加载模型、预处理、后处理，加载模型即初始化onnx会话，将模型详细传递至算力设备，预处理是将获取到的图像处理为模型可计算的图像格式，后处理是将模型推理的结果进行解算，提取出需要的有效信息。参考代码`conical.py`中`YOLOv8`类
- 构建完运行YOLO模型的类之后，可以编写运行模型的函数，`conical.py`中`model_run_conical`
- 在这段代码中，不仅有模型运行部分，还有对输入图像的图像处理和对模型输出的进一步处理。在输入图像时进行了白平衡处理和亮度修改，后处理时针对橙色和红色在不同光照环境下难以区分的问题进行了进一步的修正

3. 使用cuda进行推理时，如果在推理前后进行计时可以发现，推理一张960的图片需要6~8s的时间，对于后续运行仪表盘检测是很明显的时间浪费，但是若将设备手动指定为CPU时，会发现速度比使用CUDA更快，这不符合CUDA加速的结果，说明有了异常的时间占用。实际上是因为onnx平台在第一次加载模型时需要进行模型预热，这个预热的时间很长，那么在后续的仪表盘和数字识别中，必须让模型提前预热，方便更快的推理，参考代码`model.py`。

### 参考
https://github.com/Jadeble/sizutrain.git