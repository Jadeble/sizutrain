## 在Jetson Xavier NX中使用CUDA加速模型推理
### 实训背景
移动平台上进行本地视觉识别任务，要求设备具备高效的计算能力，以应对实时的处理需求。Jetson Xavier NX作为一款高性能嵌入式AI计算平台，集成了NVIDIA的CUDA架构，专为在功耗受限的环境中提供强大算力而设计。众所周知，使用CUDA对深度学习模型的推理进行加速，可以显著提高模型的处理速度和效率，从而满足资源受限设备的实时应用需求。
CUDA（Compute Unified Device Architecture）是NVIDIA推出的并行计算平台和编程模型，能够利用GPU的并行计算能力来加速复杂计算任务。在视觉识别任务中，模型推理通常涉及大量矩阵运算和卷积操作，而这些计算可以通过CUDA在GPU上并行执行，显著缩短处理时间。在实际应用中，使用CUDA对模型进行推理加速具有以下几个优势：

- 高效的计算能力：通过GPU的并行处理能力，CUDA能够显著提升模型推理的速度，特别是在处理复杂的卷积神经网络时效果尤为显著。
- 低功耗高性能：Jetson Xavier NX在提供强大算力的同时，保持了低功耗的特性，非常适合部署在边缘设备上，实现本地的视觉处理任务。
- 丰富的开发生态：CUDA拥有成熟的开发工具链和社区支持，开发者可以利用已有的库和框架（如`TensorFlow`、`PyTorch`等）直接在CUDA平台上运行，并进行优化。
- 超强的实时处理能力：借助CUDA加速，Jetson Xavier NX可以在本地实时处理大量数据，实现高性能的视觉识别和其他AI应用，避免了云端计算的延时和带宽消耗问题。

### 实训内容
在本次实训中，我们将深入了解如何将深度学习模型成功地部署在Jetson Xavier NX平台上，并通过CUDA进行推理加速，确保模型在实际应用中保持较高的推理速度和准确度。

具体实训内容包括以下几个方面：

1. 数据集准备
2. 模型训练
3. 模型部署
4. 推理过程的优化

### 实训目的及要求

#### 实训目的

1. 掌握边缘平台的模型部署技能

   我们将学习如何在Jetson Xavier NX平台上配置深度学习模型运行环境，并掌握将训练好的模型部署到边缘设备上的方法。这包括如何选择合适的推理框架（如ONNX）、如何进行模型转换以及如何配置设备以实现最佳性能。

2. 实现智能机器人竞赛中的识别与检测任务

   我们需要实现2024中国高校智能机器人创意大赛四足专项中的两个关键任务：锥形桶识别和仪表盘数字检测。这些任务要求模型具有高精度和快速响应能力，能够在复杂环境中准确识别目标物体并提供实时反馈。
   
3. 掌握使用CUDA加速模型推理的技术

   我们将学习如何利用CUDA对模型推理过程进行加速，充分发挥Jetson Xavier NX平台的GPU并行计算能力。在这个过程中，我们可以更好地理解CUDA加速的原理，并学会如何在实际应用中实现显著的推理速度提升。

#### 实训要求

1. 能够独立配置Jetson Xavier NX平台的开发环境，安装`opencv`和`onnxruntime-gpu`。
2. 掌握深度学习模型的优化与转换技术，能够将训练好的YOLOv8s模型从Pytorch格式转换为ONNX格式，并成功部署在平台上。
3. 实现锥形桶识别和仪表盘数字检测任务，并通过模型的推理加速优化，达到实时响应要求。同时需要对实际出现的问题调优，确保识别精度和速度的平衡。

### 实验软硬件环境
>硬件：Jetson Xavier NX开发板，摄像头（可选）
>软件：
>-  JetPack 4.4
>- Python 3.7
>- onnxruntime 1.11.0
>- opencv-python 4.7.0
>

### 实训操作指南
1. 先训练好自己用来进行识别的模型，由于当前用于视觉识别的模型大部分使用yolo，所以使用`YOLOv8s`模型为例
- 准备好拍摄的数据集，由于使用机械狗进行图像的识别，建议使用机械狗进行数据集拍摄，在拍摄过程中可以采用将识别对象进行手动旋转、更换场景增加复杂度等方式，采集合适的数据集，一般采集3k~5k张图像。
- 对采集到的数据进行增强处理，由于YOLO自带了旋转、裁剪、模糊等较为通用的数据增强手段，我们需要进行其他的数据增强方式，例如将图像进行空间扭曲，模拟不同角度观察物体；还可以将感兴趣区域裁剪后贴在其他背景图中例如`coco2017`数据集，人为创造新的环境。使用增强后的数据集训练一个`YOLOv8s`模型。

2. 部署模型
- 训练好一个模型后，就可以在机械狗上进行模型部署。首先是环境的选择，YOLOv8s本身是使用Pytorch进行推理，但是pytorch原生推理速度较慢，且在当前平台上部署`pytorch`对环境要求较为严格；`TensorRT`是Nvidia自家的加速推理框架，推理速度快，但是部署难度较高；`ONNX`部署难度低，跨平台兼容性好，推理速度稍逊于`TensorRT`，故选择ONNX作为运行模型的平台，可以用YOLO官方工具将训练所得的`pt`模型转为`onnx`模型。配置ONNX时，需要在Jetson zoo下载onnxruntime-gpu的离线安装包，以适配当前的平台。
- 运行模型需要进行如下几步：加载模型、预处理、后处理，加载模型即初始化onnx会话，将模型详细传递至算力设备，预处理是将获取到的图像处理为模型可计算的图像格式，后处理是将模型推理的结果进行解算，提取出需要的有效信息。参考代码`conical.py`中`YOLOv8`类
- 构建完运行YOLO模型的类之后，可以编写运行模型的函数，参考`conical.py`中`model_run_conical`
- 在这段代码中，不仅有模型运行部分，还有对输入图像的图像处理和对模型输出的进一步处理。在输入图像时进行了白平衡处理和亮度修改，后处理时针对橙色和红色在不同光照环境下难以区分的问题进行了进一步的修正

3. 使用CUDA进行推理时，如果在推理前后进行计时可以发现，推理一张960的图片需要6~8s的时间，对于后续运行仪表盘检测是很明显的时间浪费，但是若将设备手动指定为CPU时，会发现速度比使用CUDA更快，这不符合CUDA加速的结果，说明有了异常的时间占用。实际上是因为onnx平台在第一次加载模型时需要进行模型预热，这个预热的时间很长，那么在后续的仪表盘和数字识别中，必须让模型提前预热，方便更快的推理，参考代码`model.py`。

### 参考
https://github.com/Jadeble/sizutrain.git